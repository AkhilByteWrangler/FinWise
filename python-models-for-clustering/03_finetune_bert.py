"""
Goal: Fine-tune a BERT model to discover and separate clusters in financial transaction data,
using categories generated by a Random Forest model as supervised targets.
This approach helps uncover meaningful groupings of transaction types.

Trains with HuggingFace Transformers and saves the final model.

Saves:
- Fine-tuned BERT model
- Tokenizer
- Evaluation metrics
"""


import pandas as pd
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
from sklearn.preprocessing import LabelEncoder

# Load RF-labeled data
df = pd.read_csv("checkpoint_rf_labeled.csv")

# Encode labels
le = LabelEncoder()
df['label'] = le.fit_transform(df['MLCategory'])

# Create Hugging Face dataset
ds = Dataset.from_pandas(df[['TRANSACTION DETAILS', 'label']])
train_test = ds.train_test_split(test_size=0.2)

# Tokenization
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_test = train_test.map(lambda x: tokenizer(x['TRANSACTION DETAILS'], truncation=True), batched=True)

# BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(le.classes_))

args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=4,
    weight_decay=0.01,
    logging_dir='./logs',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='eval_loss'
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_test['train'],
    eval_dataset=train_test['test'],
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer)
)

# Train & evaluate
trainer.train()
trainer.evaluate()

# Save final model
model.save_pretrained('./bert_model')
tokenizer.save_pretrained('./bert_model')
print("Fine-tuned BERT model and tokenizer saved.")
